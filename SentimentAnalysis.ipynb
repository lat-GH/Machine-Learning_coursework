{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Load all the components we need to build this classifier\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 2: Read the data file we have. It consists of two fields\n",
    "\n",
    "tweets = pd.read_csv('c:/Users/ken/Downloads/car_reviews.csv', header = 0, encoding = 'latin-1')\n",
    "\n",
    "tweets[:3]\n",
    "\n",
    "# The tweets could do with some clean up such as removing punctuation, capitalisation, stop words. And stemming or lemmatisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 3: Extract the fields\n",
    "\n",
    "tweet_text, tweet_labels = tweets.TweetText, tweets.Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 4: Set up the natural language toolkit tokenizer and the counter\n",
    "\n",
    "# It has a tokeniser designed for tweets.\n",
    "\n",
    "tokenizer = nltk.casual.TweetTokenizer(preserve_case=False, reduce_len=True)\n",
    "\n",
    "# We will count the number of each word in the corpus and set frequency limits to reduce overall size.\n",
    "\n",
    "vectoriser = CountVectorizer(tokenizer=tokenizer.tokenize, min_df = 0.007, max_df = 0.991)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 5: Build the corpus\n",
    "\n",
    "corpus = tweet_text.tolist() #we should really only use the training data and ignore new words in the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 6: Apply the counter/vectoriser\n",
    "\n",
    "tweet_fitted = vectoriser.fit_transform(corpus)\n",
    "\n",
    "features = len(vectoriser.get_feature_names())\n",
    "\n",
    "print('Number of features', features)\n",
    "print(tweet_fitted.toarray()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 7: Split the input data into training and test data. 80%/20%\n",
    "\n",
    "train_tweets, test_tweets, train_labels, test_labels = train_test_split(\n",
    "    tweet_fitted, tweet_labels, random_state=48746, test_size = 0.2)\n",
    "\n",
    "totaltweets = train_labels.count()\n",
    "positives = train_labels.sum()\n",
    "negatives = totaltweets - positives\n",
    "\n",
    "\n",
    "print('p(1) =', positives/totaltweets)\n",
    "print('p(0) =', negatives/totaltweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 8: Create the model\n",
    "\n",
    "model = MultinomialNB(fit_prior=True)\n",
    "\n",
    "# fit the model using the training data's labels\n",
    "\n",
    "model.fit(train_tweets, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 9: Apply the model\n",
    "\n",
    "test_pred = model.predict(test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 10: Get the confusion matrix of the predictions vs the ground truth of the test data.\n",
    "\n",
    "conf_mat = confusion_matrix(test_labels, test_pred)\n",
    "\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 11: Sum the true predictions and divide by total predictions to calculate precentage correct.\n",
    "\n",
    "true_neg, false_pos, false_neg, true_pos = conf_mat.ravel()\n",
    "\n",
    "total_tweets = (true_neg + false_pos + false_neg + true_pos)\n",
    "\n",
    "correctly_predicted = ((true_pos + true_neg) / total_tweets) * 100\n",
    "\n",
    "print(correctly_predicted, '% correctly predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
